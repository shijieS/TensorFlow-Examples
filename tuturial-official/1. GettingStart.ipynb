{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With TensorFlow\n",
    "Before start, you should know the following things\n",
    "- How to program in Python.\n",
    "- At least a little bit about **arrays**.\n",
    "- Ideally, **something about machine learning**. However, if you know little or nothing about machine learning, then this is still the first guide you should read.\n",
    "\n",
    "Tensorflow provide multiple API\n",
    "- TensorFlow Core\n",
    "    - provide with you the complete programming control\n",
    "    - recommended for starter\n",
    "    - The higher level APIs are built on top of TensorFlow Core\n",
    "- tf.estimator\n",
    "    - helps you manage \n",
    "        - data sets\n",
    "        - estimators\n",
    "        - training\n",
    "        - inference\n",
    "        - implement the same model\n",
    "        \n",
    "Knowing TensorFlow Core principles will give you a great **mental model** of how things are working internally when you use the more compact higher level API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "The central unit of data in TensorFlow is the **tensor**. A tensor consists of a set of **primitive values** shaped into an array of any number of dimensions. A **tensor's rank** is its number of dimensions. Here are some examples of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3 # a rank 0 tensor; a scalar with shape []\n",
    "[1., 2., 3.] # a rank 1 tensor; a vector with shape [3]\n",
    "[[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3]\n",
    "[[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow core tuturial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing\n",
    "The canonical import statement for TensorFlow programs is as follows. This gives Python access to all of TensorFlow's **classes, methods, and symbols**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational Graphic\n",
    "You might think of TensorFlow Core programs as consisting of **two discrete sections**:\n",
    "- **Building** the computational graph.\n",
    "- **Running** the computational graph.\n",
    "\n",
    "A **computational graph** is a series of **TensorFlow operations** arranged into a **graph of nodes**. Let's build a simple computational graph. Each node takes zero or more tensors as inputs and produces a tensor as an output. One type of node is a constant. Like all TensorFlow constants, it takes no inputs, and it outputs a value it stores internally. We can create two floating point Tensors node1 and node2 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicityly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that printing the nodes does not output the values 3.0 and 4.0 as you might expect. Instead, they are **nodes** that, when evaluated, would produce 3.0 and 4.0, respectively. To actually evaluate the nodes, we must **run the computational graph within a session**. A **session** encapsulates the *control and state* of the TensorFlow runtime.\n",
    "\n",
    "The following code creates a Session object and then invokes its run method to **run enough of the computational graph** to evaluate node1 and node2. By running the computational graph in a session as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build more complicated computations by combining Tensor nodes with operations (Operations are also nodes). For example, we can **add our two constant nodes** and produce a new graph as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3 Tensor(\"Add:0\", shape=(), dtype=float32)\n",
      "session node3:  7.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3\", node3)\n",
    "print(\"session node3: \", sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides a utility called **TensorBoard** that can *display a picture of the computational graph*. Here is a screenshot showing how **TensorBoard visualizes the graph**:\n",
    "\n",
    "As it stands, this graph is not especially interesting because it always produces a constant result. A graph can be parameterized to **accept external inputs**, known as **placeholders**. A placeholder is a **promise to provide a value later**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a+b # + provide a shortcut for tf.add(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding three lines are a bit like **a function or a lambda** in which we define two input parameters (a and b) and then an operation on them. We can evaluate this graph with **multiple inputs** by using the **feed_dict argument** to the run method to feed concrete values to the placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "[ 4.  6.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, feed_dict={a:1.0, b:2.0}))\n",
    "print(sess.run(adder_node, feed_dict={a:[1.0, 2.0], b:[3.0, 4.0]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "adder_node_triple = adder_node*3\n",
    "print(sess.run(adder_node_triple, feed_dict={a:1.0, b:2.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning we will typically want a model that can **take arbitrary inputs**, such as the one above. To make the model **trainable**, we need to be able to **modify the graph** to get new outputs with the same input. Variables allow us to add trainable parameters to a graph. They are **constructed with a type and initial value**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constants are initialized** when you call tf.constant, and their value can never change. By contrast, **variables are not initialized** when you call tf.Variable. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to realize **init is a handle to the TensorFlow sub-graph** that initializes all the global variables. Until we call sess.run, the variables are uninitialized.\n",
    "\n",
    "Since x is a placeholder, we can **evaluate linear_model for several values of x** simultaneously as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, feed_dict={x:[1., 2., 3., 4.]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a model, but we don't know how good it is yet. To evaluate the model on training data, we need a **y placeholder** to provide the **desired values**, and we need to write a **loss function**.\n",
    "\n",
    "A loss function measures how far apart the current model is from the provided data. We'll use a standard loss model for **linear regression**, which **sums the squares of the deltas** between the current model and the provided data. linear_model - y creates a vector where each element is the corresponding example's error delta. We call **tf.square** to square that error. Then, **we sum all the squared errors** to create a **single scalar** that abstracts the error of all examples using **tf.reduce_sum**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(dtype=tf.float32)\n",
    "squar_delta = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squar_delta)\n",
    "print(sess.run(loss, {x:[1, 2, 3, 4], y:[0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could improve this manually by **reassigning the values** of $W$ and $b$ to the perfect values of -1 and 1. A variable is initialized to the value provided to **tf.Variable** but can be changed using operations like **tf.assign**. For example, $W=-1$ and $b=1$ are the optimal parameters for our model. We can change $W$ and $b$ accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x:[1, 2, 3, 4], y:[0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We guessed the \"**perfect**\" values of $W$ and $b$, but the whole point of machine learning is to **find the correct model parameters automatically**. We will show how to accomplish this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train API\n",
    "A complete discussion of machine learning is out of the scope of this tutorial. However, TensorFlow provides **optimizers** that slowly change each variable in order to **minimize the loss function**. The simplest optimizer is **gradient descent**. It modifies each variable according to the **magnitude of the derivative** of loss with respect to that variable. In general, computing **symbolic derivatives manually** is tedious and error-prone. Consequently, TensorFlow can **automatically produce derivatives** given only a description of the model using the function **tf.gradients**. For simplicity, optimizers typically do this for you. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9999969] [ 0.99999082] 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "sess.run(init)\n",
    "for i in range(1000):\n",
    "    sess.run(train, feed_dict={x:[1, 2, 3, 4], y:[0, -1, -2, -3]})\n",
    "    \n",
    "print(sess.run(W), sess.run(b), sess.run(loss, feed_dict={x:[1, 2, 3, 4], y:[0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have done actual machine learning! Although this simple linear regression model does not require much TensorFlow core code, more complicated models and methods to feed data into your models **necessitate more code**. Thus, TensorFlow provides **higher level abstractions for common patterns, structures, and functionality**. We will learn how to use some of these **abstractions** in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the **loss** is a very small number (**very close to zero**). If you run this program, your loss may not be exactly **the same** as the aforementioned loss because the model is initialized with pseudorandom values.\n",
    "\n",
    "This more complicated program can still be visualized in TensorBoard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.estimator API\n",
    "**tf.estimator** is a **high-level TensorFlow library** that simplifies the mechanics of machine learning, including the following:\n",
    "- running **training** loops\n",
    "- running **evaluation** loops\n",
    "- **managing data sets**\n",
    "\n",
    "tf.estimator defines **many common models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Usage\n",
    "Notice how much simpler the linear regression program becomes with tf.estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\00097307\\AppData\\Local\\Temp\\tmp44rbsjku\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\00097307\\\\AppData\\\\Local\\\\Temp\\\\tmp44rbsjku', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A842A22860>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\00097307\\AppData\\Local\\Temp\\tmp44rbsjku\\model.ckpt.\n",
      "INFO:tensorflow:loss = 12.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1514.08\n",
      "INFO:tensorflow:loss = 0.313483, step = 101 (0.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 1998.6\n",
      "INFO:tensorflow:loss = 0.0263219, step = 201 (0.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 1818.97\n",
      "INFO:tensorflow:loss = 0.0105334, step = 301 (0.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 1774.78\n",
      "INFO:tensorflow:loss = 0.00105439, step = 401 (0.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 1833.56\n",
      "INFO:tensorflow:loss = 0.000295055, step = 501 (0.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 1753.15\n",
      "INFO:tensorflow:loss = 5.03711e-05, step = 601 (0.057 sec)\n",
      "INFO:tensorflow:global_step/sec: 1867.85\n",
      "INFO:tensorflow:loss = 9.5473e-06, step = 701 (0.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 1900.94\n",
      "INFO:tensorflow:loss = 1.12639e-06, step = 801 (0.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 1819.95\n",
      "INFO:tensorflow:loss = 2.74636e-07, step = 901 (0.055 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\00097307\\AppData\\Local\\Temp\\tmp44rbsjku\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 7.66683e-08.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-15-15:54:24\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\00097307\\AppData\\Local\\Temp\\tmp44rbsjku\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-15-15:54:25\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 1.24676e-08, global_step = 1000, loss = 4.98705e-08\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-15-15:54:25\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\00097307\\AppData\\Local\\Temp\\tmp44rbsjku\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-15-15:54:26\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.0025347, global_step = 1000, loss = 0.0101388\n",
      "train metrics: {'average_loss': 1.2467614e-08, 'loss': 4.9870458e-08, 'global_step': 1000}\n",
      "eval metrics: {'average_loss': 0.0025346989, 'loss': 0.010138796, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "# NumPy is often used to load, manipulate and preprocess data.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Declare list of features. We only have one numeric feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1])]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# linear classification, and many neural network classifiers and regressors.\n",
    "# The following code provides an estimator that does linear regression.\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the  method and passing the\n",
    "# training data set.\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A custom model\n",
    "tf.estimator does not lock you into its predefined models. Suppose we wanted to **create a custom model** that is not built into TensorFlow. We can still **retain the high level abstraction of data set, feeding, training, etc. of tf.estimator**. For illustration, we will show how to implement our **own equivalent model** to *LinearRegressor* using our knowledge of the lower level TensorFlow API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\00097307\\AppData\\Local\\Temp\\tmpq2ye6s2z\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\00097307\\\\AppData\\\\Local\\\\Temp\\\\tmpq2ye6s2z', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A8448126A0>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\00097307\\AppData\\Local\\Temp\\tmpq2ye6s2z\\model.ckpt.\n",
      "INFO:tensorflow:loss = 117.75230374, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1664.84\n",
      "INFO:tensorflow:loss = 0.0770135507025, step = 101 (0.062 sec)\n",
      "INFO:tensorflow:global_step/sec: 1784.47\n",
      "INFO:tensorflow:loss = 0.00239547810298, step = 201 (0.056 sec)\n",
      "INFO:tensorflow:global_step/sec: 1804.31\n",
      "INFO:tensorflow:loss = 0.000939679191478, step = 301 (0.055 sec)\n",
      "INFO:tensorflow:global_step/sec: 1867.81\n",
      "INFO:tensorflow:loss = 0.000109082857772, step = 401 (0.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 1998.61\n",
      "INFO:tensorflow:loss = 4.86741517925e-06, step = 501 (0.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 1998.58\n",
      "INFO:tensorflow:loss = 1.77772535214e-07, step = 601 (0.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 2018.74\n",
      "INFO:tensorflow:loss = 3.16159282075e-08, step = 701 (0.052 sec)\n",
      "INFO:tensorflow:global_step/sec: 2039.4\n",
      "INFO:tensorflow:loss = 1.27141852124e-09, step = 801 (0.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 1903.42\n",
      "INFO:tensorflow:loss = 5.39823614759e-10, step = 901 (0.053 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into C:\\Users\\00097307\\AppData\\Local\\Temp\\tmpq2ye6s2z\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.71163475627e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-15-16:02:58\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\00097307\\AppData\\Local\\Temp\\tmpq2ye6s2z\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-15-16:02:58\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 3.66594e-11\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-15-16:02:59\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\00097307\\AppData\\Local\\Temp\\tmpq2ye6s2z\\model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-15-16:02:59\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.0101013\n",
      "train metrics: {'loss': 3.6659367e-11, 'global_step': 1000}\n",
      "eval metrics: {'loss': 0.010101295, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Declare list of features, we only have one real-valued feature\n",
    "def model_fn(features, labels, mode):\n",
    "  # Build a linear model and predict values\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W*features['x'] + b\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "  # EstimatorSpec connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7., 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# train\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
